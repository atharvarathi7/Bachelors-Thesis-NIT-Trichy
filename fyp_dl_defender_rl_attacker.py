# -*- coding: utf-8 -*-
"""FYP_DL_Defender_RL_Attacker.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jB6mBAiZNpkkGARgl21N03nx_VsD3eSC
"""

pip install pandapower

import numpy as np
import gym
from gym import spaces
import torch
import torch.nn as nn
import torch.optim as optim
from torch.distributions import Normal
from collections import deque, namedtuple
import random
import matplotlib.pyplot as plt
import pandapower as pp

def calculate_q_v_delta_bfs_simplified(p_values):
        """
        Calculate reactive power (Q), voltage magnitude (V), and voltage angle (delta)
        for a simplified network where all load buses are directly connected to the slack bus.

        Parameters:
        p_values (numpy.ndarray): Active power values for all load buses

        Returns:
        tuple: (Q_values, V_values, delta_values)
        """
        if not isinstance(p_values, np.ndarray):
            raise ValueError("p_values must be a numpy array")

        # Create an empty network
        net = pp.create_empty_network()

        # Add a slack bus (within the DSO)
        slack_bus = pp.create_bus(net, vn_kv=230, name="Slack Bus")
        pp.create_ext_grid(net, bus=slack_bus, vm_pu=1.0, name="Grid Connection")

        # Create load buses
        buses = [pp.create_bus(net, vn_kv=230, name=f"Bus {i+1}") for i in range(len(p_values))]

        # Connect each load bus directly to the slack bus
        for i, bus in enumerate(buses):
            pp.create_line_from_parameters(
                net, from_bus=slack_bus, to_bus=bus, length_km=1,
                r_ohm_per_km=0.02, x_ohm_per_km=0.06, c_nf_per_km=0, max_i_ka=0.5,
                name=f"Slack to Bus {i+1}"
            )

        # Calculate total active power demand
        total_p = np.sum(p_values[p_values > 0])

        # Assume constant power factor of 0.5
        power_factor = 0.5
        q_factor = np.tan(np.arccos(power_factor))

        # Add loads to the buses
        for i, p_mw in enumerate(p_values):
            if p_mw <= 0:
                continue  # Skip buses with non-positive power

            q_mvar = p_mw * q_factor

            pp.create_load(net, bus=buses[i], p_mw=p_mw, q_mvar=q_mvar, name=f"Load at Bus {i+1}")

        # Set slack bus power injection
        pp.create_gen(net, bus=slack_bus, p_mw=total_p, vm_pu=1.0, name="Slack Generator")

        # Run power flow
        try:
            pp.runpp(net)
        except Exception as e:
            print(f"Power flow calculation failed: {e}")
            return (
                np.full_like(p_values, np.nan),
                np.full_like(p_values, np.nan),
                np.full_like(p_values, np.nan),
            )

        # Extract results for load buses only
        load_bus_indices = [bus for bus in buses]
        Q_values = net.res_load.q_mvar.values if "res_load" in net else np.zeros_like(p_values)
        V_values = net.res_bus.vm_pu.loc[load_bus_indices].values
        delta_values = net.res_bus.va_degree.loc[load_bus_indices].values

        return Q_values, V_values, delta_values

# Define a neural network-based defender
class PowerSystemDefender(nn.Module):
    """
    Neural network-based defender that detects anomalies in power system measurements.
    The defender flags suspicious patterns that may indicate a false data injection attack.
    """
    def __init__(self, input_dim, hidden_dim=64):
        super(PowerSystemDefender, self).__init__()
        self.model = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, 1),
            nn.Sigmoid()  # Output probability of attack between 0 and 1
        )
        self.optimizer = optim.Adam(self.parameters(), lr=1e-3)
        self.loss_fn = nn.BCELoss()

        # Detection threshold
        self.detection_threshold = 0.5

        # Training history
        self.loss_history = []
        self.training_buffer = deque(maxlen=1000)

    def forward(self, x):
        return self.model(x)

    def detect_attack(self, p_values, original_p_values, q_values, original_q_values,
                     v_values, original_v_values, delta_values, original_delta_values):
        """
        Detect if the current state represents an attack
        Returns: probability of attack, boolean flag if attack is detected
        """
        # Create a feature vector from various measurements and their differences
        features = torch.FloatTensor([
            *p_values,
            *q_values,
            *v_values,
            *delta_values,
            *np.abs(p_values - original_p_values),
            *np.abs(q_values - original_q_values),
            *np.abs(v_values - original_v_values),
            *np.abs(delta_values - original_delta_values),
            np.sum(p_values),
            np.sum(q_values),
            np.mean(v_values),
            np.std(delta_values)
        ])

        with torch.no_grad():
            attack_probability = self.forward(features)

        # Flag as attack if probability exceeds threshold
        is_attack = bool(attack_probability.item() > self.detection_threshold)

        return attack_probability.item(), is_attack

    def store_sample(self, p_values, original_p_values, q_values, original_q_values,
                     v_values, original_v_values, delta_values, original_delta_values, is_attack):
        """
        Store a sample for training the defender
        """
        features = torch.FloatTensor([
            *p_values,
            *q_values,
            *v_values,
            *delta_values,
            *np.abs(p_values - original_p_values),
            *np.abs(q_values - original_q_values),
            *np.abs(v_values - original_v_values),
            *np.abs(delta_values - original_delta_values),
            np.sum(p_values),
            np.sum(q_values),
            np.mean(v_values),
            np.std(delta_values)
        ])

        label = torch.FloatTensor([float(is_attack)])
        self.training_buffer.append((features, label))

    def train_step(self, batch_size=32):
        """
        Train the defender on a batch of samples
        """
        if len(self.training_buffer) < batch_size:
            return None

        # Sample a batch
        batch = random.sample(self.training_buffer, batch_size)
        features = torch.stack([item[0] for item in batch])
        labels = torch.stack([item[1] for item in batch])

        # Forward pass
        self.optimizer.zero_grad()
        outputs = self.forward(features)
        loss = self.loss_fn(outputs, labels)

        # Backward pass
        loss.backward()
        self.optimizer.step()

        self.loss_history.append(loss.item())
        return loss.item()


class PredictedPAttackEnvWithDefender(gym.Env):
    """
    Environment for False Data Injection Attack (FDIA) on predicted P values,
    now including a defender that can detect attacks.
    """

    def __init__(self, initial_p_values, target_indices=None, episode_length=100, enable_defender=True):
        super(PredictedPAttackEnvWithDefender, self).__init__()

        # Store the predicted P values
        self.original_p_values = np.array(initial_p_values, dtype=np.float32)
        self.current_p_values = self.original_p_values.copy()

        # If no target indices provided, target all values
        if target_indices is None:
            self.target_indices = list(range(len(initial_p_values)))
        else:
            self.target_indices = target_indices

        self.num_targets = len(self.target_indices)
        self.episode_length = episode_length
        self.current_step = 0
        self.enable_defender = enable_defender

        # Calculate total power
        self.total_power = np.sum(self.original_p_values)

        # Define action space: inject false data to each target (percentage change)
        # Actions between -0.01 (1% decrease) and 0.01 (1% increase)
        self.action_space = spaces.Box(
            low=-0.01,
            high=0.01,
            shape=(self.num_targets,),
            dtype=np.float32
        )

        # Define observation space: current P values
        self.observation_space = spaces.Box(
            low=0,
            high=np.inf,
            shape=(self.num_targets,),
            dtype=np.float32
        )

        # Tracking metrics
        self.attack_success_history = []
        self.reward_history = []
        self.defender_detection_history = []

        # Initialize the defender
        # Input dim includes p, q, v, delta values, their differences, and aggregated statistics
        input_dim = 4 * len(initial_p_values) + 4 * len(initial_p_values) + 4
        self.defender = PowerSystemDefender(input_dim)

        # Calculate initial Q, V, and delta values
        self.original_q_values, self.original_v_values, self.original_delta_values = self.calculate_q_v_delta()
        self.current_q_values = self.original_q_values.copy()
        self.current_v_values = self.original_v_values.copy()
        self.current_delta_values = self.original_delta_values.copy()

    def reset(self):
        """Reset the environment to initial state"""
        self.current_step = 0
        self.current_p_values = self.original_p_values.copy()
        self.current_q_values = self.original_q_values.copy()
        self.current_v_values = self.original_v_values.copy()
        self.current_delta_values = self.original_delta_values.copy()

        # Return the initial observation (just the target values)
        return self._get_observation()

    def _get_observation(self):
        """Get current P values for targets"""
        return self.current_p_values[self.target_indices].copy()


    def calculate_q_v_delta(self):
        """Calculate Q, V, and delta values based on current P values"""
        return calculate_q_v_delta_bfs_simplified(self.current_p_values)

    def _calculate_reward(self):
        """
        Calculate reward based on:
        1. How much the injected values differ from true values (stealth)
        2. Maintaining total power conservation
        3. Avoiding detection by the defender
        """
        # Extract current values at target indices
        current_targets = self.current_p_values[self.target_indices]
        original_targets = self.original_p_values[self.target_indices]

        # Determine maximum allowable change without detection (assumed to be 15%)
        max_allowed_deviation = 0.15 * original_targets

        # Calculate how close we are to the detection threshold
        detection_closeness = np.abs(current_targets - original_targets) / max_allowed_deviation

        # Penalty for exceeding detection threshold
        stealth_reward = 5 * np.sum(detection_closeness <= 0.1)
        detection_penalty = -25 * (self.current_step / self.episode_length) * np.sum(np.maximum(detection_closeness - 1.0, 0))

        # Reward for successful manipulation (how much we've changed values)
        manipulation_reward = 20 * np.sum(np.abs(current_targets - original_targets)) / np.sum(original_targets)

        # Check conservation of total power
        total_current_power = np.sum(self.current_p_values)
        power_conservation_penalty = -50 * abs(total_current_power - self.total_power) / self.total_power

        # Defender reward/penalty (if enabled)
        defender_penalty = 0
        if self.enable_defender:
            # Get defender's assessment
            attack_probability, is_attack = self.defender.detect_attack(
                self.current_p_values, self.original_p_values,
                self.current_q_values, self.original_q_values,
                self.current_v_values, self.original_v_values,
                self.current_delta_values, self.original_delta_values
            )

            # Store detection result
            self.defender_detection_history.append(is_attack)

            # Large penalty if defender detects an attack
            if is_attack:
                defender_penalty = -200 * attack_probability

            # True label: attack is happening if values deviate more than 15%
            is_real_attack = np.any(detection_closeness > 1.0)

            # Store sample for training
            self.defender.store_sample(
                self.current_p_values, self.original_p_values,
                self.current_q_values, self.original_q_values,
                self.current_v_values, self.original_v_values,
                self.current_delta_values, self.original_delta_values,
                is_real_attack
            )

            # Train step
            self.defender.train_step()

        # Traditional success criteria
        power_system_success = (np.all(detection_closeness <= 1.0) and
                         abs(total_current_power - self.total_power) / self.total_power < 0.01)

        # New success criteria: evaded defender detection as well
        attack_success = power_system_success
        if self.enable_defender and len(self.defender_detection_history) > 0:
            attack_success = attack_success and not self.defender_detection_history[-1]

        reward = 0

        if attack_success:
            reward += 100

        else:
            # Combine rewards
            reward = manipulation_reward + stealth_reward + detection_penalty + power_conservation_penalty + defender_penalty

        # Record if attack was successful
        self.attack_success_history.append(attack_success)
        self.reward_history.append(reward)

        return reward

    def step(self, action):
        """
        Execute an attack action
        Action: percentage changes to apply to each target P value
        """
        self.current_step += 1

        # Make a copy of the current values for later comparison
        prev_values = self.current_p_values.copy()

        # Apply actions to all target indices
        for i, idx in enumerate(self.target_indices):
            current_power = self.current_p_values[idx]
            power_change = current_power * action[i]

            # Apply the change, ensuring power remains positive
            new_power = current_power + power_change
            if new_power > 0:
                self.current_p_values[idx] = new_power
            else:
                # If action would make power negative, don't apply it
                pass

        # Adjust power values proportionally to maintain total power
        total_current_power = np.sum(self.current_p_values)
        power_difference = self.total_power - total_current_power

        # Distribute the power difference proportionally across all targets
        for i, idx in enumerate(self.target_indices):
            self.current_p_values[idx] += power_difference * (self.current_p_values[idx] / total_current_power)

        # Ensure all power values are positive
        self.current_p_values[self.current_p_values < 0] = 0.01

        # Calculate new Q, V, and delta values
        self.current_q_values, self.current_v_values, self.current_delta_values = self.calculate_q_v_delta()

        # Get new observation
        new_observation = self._get_observation()

        # Calculate reward
        reward = self._calculate_reward()

        # Check if episode is done
        done = self.current_step >= self.episode_length

        return new_observation, reward, done, {
            "current_p_values": self.current_p_values.copy(),
            "current_q_values": self.current_q_values,
            "current_v_values": self.current_v_values,
            "current_delta_values": self.current_delta_values,
            "defender_detection": self.defender_detection_history[-1] if self.enable_defender and len(self.defender_detection_history) > 0 else False
        }

class Actor(nn.Module):
    """Actor network for the PPO algorithm"""

    def __init__(self, state_dim, action_dim, hidden_dim=64):
        super(Actor, self).__init__()
        self.actor = nn.Sequential(
            nn.Linear(state_dim, hidden_dim),
            nn.Tanh(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.Tanh()
        )

        # Mean and log_std are separate outputs
        self.mean = nn.Linear(hidden_dim, action_dim)
        self.log_std = nn.Parameter(torch.zeros(action_dim))

    def forward(self, state):
        x = self.actor(state)
        mean = self.mean(x)

        # Ensure mean is within bounds
        mean = torch.clamp(mean, -0.5, 0.5)

        std = torch.exp(self.log_std)
        return mean, std


class Critic(nn.Module):
    """Critic network for the PPO algorithm"""

    def __init__(self, state_dim, hidden_dim=64):
        super(Critic, self).__init__()
        self.critic = nn.Sequential(
            nn.Linear(state_dim, hidden_dim),
            nn.Tanh(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.Tanh(),
            nn.Linear(hidden_dim, 1)
        )

    def forward(self, state):
        return self.critic(state)


class PPO:
    """Proximal Policy Optimization algorithm"""

    def __init__(self, state_dim, action_dim, lr=3e-4, gamma=0.99, clip_ratio=0.2,
                 value_coef=0.5, entropy_coef=0.01, max_grad_norm=0.5,
                 batch_size=64, buffer_size=2000):
        self.gamma = gamma
        self.clip_ratio = clip_ratio
        self.value_coef = value_coef
        self.entropy_coef = entropy_coef
        self.max_grad_norm = max_grad_norm
        self.batch_size = batch_size

        # Initialize actor and critic networks
        self.actor = Actor(state_dim, action_dim)
        self.critic = Critic(state_dim)

        # Initialize optimizers
        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=lr)
        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=lr)

        # Experience buffer
        self.buffer = deque(maxlen=buffer_size)

        # Training metrics
        self.actor_losses = []
        self.critic_losses = []
        self.entropy_losses = []

    def select_action(self, state):
        """Select an action using the current policy"""
        state = torch.FloatTensor(state)
        with torch.no_grad():
            mean, std = self.actor(state)

        # Create a Normal distribution and sample from it
        dist = Normal(mean, std)
        action = dist.sample()

        # Calculate log probability
        log_prob = dist.log_prob(action).sum(dim=-1, keepdim=True)

        # Clip action to be within allowed range
        action = torch.clamp(action, -0.05, 0.05)

        return action.numpy(), log_prob.numpy()

    def store_transition(self, transition):
        """Store a transition in the replay buffer"""
        self.buffer.append(transition)

    def update(self):
        """Update the policy and value networks using PPO"""
        if len(self.buffer) < self.batch_size:
            return

        # Sample batch
        batch = random.sample(self.buffer, self.batch_size)

        # Extract batch data
        states = torch.FloatTensor(np.array([t.state for t in batch]))
        actions = torch.FloatTensor(np.array([t.action for t in batch]))
        old_log_probs = torch.FloatTensor(np.array([t.log_prob for t in batch]))
        rewards = torch.FloatTensor(np.array([t.reward for t in batch]))
        next_states = torch.FloatTensor(np.array([t.next_state for t in batch]))
        dones = torch.FloatTensor(np.array([t.done for t in batch]))

        # Compute returns
        returns = []
        discounted_sum = 0
        for reward, done in zip(reversed(rewards), reversed(dones)):
            if done:
                discounted_sum = 0
            discounted_sum = reward + self.gamma * discounted_sum
            returns.insert(0, discounted_sum)

        returns = torch.FloatTensor(returns)

        # Normalize returns
        returns = (returns - returns.mean()) / (returns.std() + 1e-8)

        # Update for multiple epochs
        for _ in range(5):
            # Get current policy distribution
            mean, std = self.actor(states)
            dist = Normal(mean, std)

            # Get log probabilities of actions
            curr_log_probs = dist.log_prob(actions).sum(dim=1)

            # Calculate entropy
            entropy = dist.entropy().mean()

            # Get current value estimates
            values = self.critic(states).squeeze()

            # Compute advantage
            advantages = returns - values.detach()

            # Compute PPO ratio
            ratio = torch.exp(curr_log_probs - old_log_probs)

            # Compute PPO losses
            surr1 = ratio * advantages
            surr2 = torch.clamp(ratio, 1.0 - self.clip_ratio, 1.0 + self.clip_ratio) * advantages

            # Policy loss
            actor_loss = -torch.min(surr1, surr2).mean()

            # Value loss
            critic_loss = ((values - returns) ** 2).mean()

            # Total loss
            loss = actor_loss + self.value_coef * critic_loss - self.entropy_coef * entropy

            # Update networks
            self.actor_optimizer.zero_grad()
            self.critic_optimizer.zero_grad()
            loss.backward()

            # Clip gradient norm
            torch.nn.utils.clip_grad_norm_(self.actor.parameters(), self.max_grad_norm)
            torch.nn.utils.clip_grad_norm_(self.critic.parameters(), self.max_grad_norm)

            self.actor_optimizer.step()
            self.critic_optimizer.step()

            # Store losses for tracking
            self.actor_losses.append(actor_loss.item())
            self.critic_losses.append(critic_loss.item())
            self.entropy_losses.append(entropy.item())

    def save(self, path):
        """Save model parameters"""
        torch.save({
            'actor': self.actor.state_dict(),
            'critic': self.critic.state_dict(),
            'actor_optimizer': self.actor_optimizer.state_dict(),
            'critic_optimizer': self.critic_optimizer.state_dict(),
        }, path)

    def load(self, path):
        """Load model parameters"""
        checkpoint = torch.load(path)
        self.actor.load_state_dict(checkpoint['actor'])
        self.critic.load_state_dict(checkpoint['critic'])
        self.actor_optimizer.load_state_dict(checkpoint['actor_optimizer'])
        self.critic_optimizer.load_state_dict(checkpoint['critic_optimizer'])

# The rest of the classes (Actor, Critic, PPO) remain the same

# Define a new class for the defender agent
class DefenderAgent:
    """
    Agent that learns to detect False Data Injection Attacks by observing
    power system measurements.
    """

    def __init__(self, input_dim, hidden_dim=64, lr=1e-3):
        self.defender = PowerSystemDefender(input_dim, hidden_dim)

    def detect_attack(self, p_values, original_p_values, q_values, original_q_values,
                     v_values, original_v_values, delta_values, original_delta_values):
        """Interface to the defender's detection method"""
        return self.defender.detect_attack(
            p_values, original_p_values, q_values, original_q_values,
            v_values, original_v_values, delta_values, original_delta_values
        )

    def train(self, p_values, original_p_values, q_values, original_q_values,
             v_values, original_v_values, delta_values, original_delta_values, is_attack):
        """Interface to the defender's training methods"""
        self.defender.store_sample(
            p_values, original_p_values, q_values, original_q_values,
            v_values, original_v_values, delta_values, original_delta_values, is_attack
        )
        return self.defender.train_step()

# Define a named tuple for storing transitions
Transition = namedtuple('Transition',
                        ('state', 'action', 'log_prob', 'reward', 'next_state', 'done'))

def train_agent(env, agent, episodes=1000, max_steps=100, print_interval=10):
    """Train the agent"""
    rewards = []
    success_rates = []

    for episode in range(episodes):
        state = env.reset()
        episode_reward = 0

        for step in range(max_steps):
            # Select action
            action, log_prob = agent.select_action(state)

            # Take action
            next_state, reward, done, _ = env.step(action)

            # Store transition
            transition = Transition(state, action, log_prob, reward, next_state, done)
            agent.store_transition(transition)

            # Update agent
            agent.update()

            # Update state and reward
            state = next_state
            episode_reward += reward

            if done:
                break

        # Track performance
        rewards.append(episode_reward)

        # Calculate success rate for this episode
        if len(env.attack_success_history) > 0:
            recent_success = np.mean(env.attack_success_history[-max_steps:])
            success_rates.append(recent_success)

        # Print progress
        if (episode + 1) % print_interval == 0:
            avg_reward = np.mean(rewards[-print_interval:])
            if len(success_rates) >= print_interval:
                avg_success = np.mean(success_rates[-print_interval:])
                print(f"Episode {episode+1}, Avg Reward: {avg_reward:.2f}, Success Rate: {avg_success:.2%}")
            else:
                print(f"Episode {episode+1}, Avg Reward: {avg_reward:.2f}")

    return rewards, success_rates

def evaluate_agent(env, agent, episodes=10):
    """Evaluate the trained agent"""
    print("\nEvaluating agent...")
    eval_rewards = []
    eval_success = []

    for episode in range(episodes):
        state = env.reset()
        episode_reward = 0
        success_count = 0
        steps = 0

        while True:
            # Select action
            action, _ = agent.select_action(state)

            # Take action
            next_state, reward, done, info = env.step(action)

            # Update state and metrics
            state = next_state
            episode_reward += reward
            steps += 1

            # Check if attack was successful
            if len(env.attack_success_history) > 0 and env.attack_success_history[-1]:
                success_count += 1

            if done:
                break

        eval_rewards.append(episode_reward)
        if steps > 0:
            eval_success.append(success_count / steps)

        print(f"Episode {episode+1}, Reward: {episode_reward:.2f}, Success Rate: {success_count/steps:.2%}")

    print(f"\nAverage Evaluation Reward: {np.mean(eval_rewards):.2f}")
    print(f"Average Success Rate: {np.mean(eval_success):.2%}")

    return eval_rewards, eval_success

# Modified main function
def main_with_defender():
    """Main function to run the training with defender mechanism"""
    # Sample predicted P values
    predicted_p_values = np.array([10.0, 12.5, 8.0, 15.2, 9.8])

    # Create environment with defender
    target_indices = [0, 1, 2, 3, 4]  # Target all values
    env = PredictedPAttackEnvWithDefender(
        predicted_p_values,
        target_indices=target_indices,
        episode_length=50,
        enable_defender=True  # Enable the defender
    )

    # Create agent
    state_dim = env.observation_space.shape[0]
    action_dim = env.action_space.shape[0]
    agent = PPO(state_dim, action_dim)

    # Train agent
    rewards, success_rates = train_agent(env, agent, episodes=100, max_steps=50, print_interval=20)

    # Evaluate agent
    eval_rewards, eval_success = evaluate_agent(env, agent)

    # Calculate metrics for original P values
    original_Q, original_V, original_delta = calculate_q_v_delta_bfs_simplified(predicted_p_values)

    # Calculate metrics for attacked P values
    attacked_Q, attacked_V, attacked_delta = calculate_q_v_delta_bfs_simplified(env.current_p_values)

    # Visualize results with all parameters
    visualize_results_with_defender(
        rewards,
        success_rates,
        env,
        predicted_p_values,
        original_Q,
        original_V,
        original_delta,
        env.current_p_values,
        attacked_Q,
        attacked_V,
        attacked_delta
    )

    # Print the final results
    print("\nOriginal P Values:", predicted_p_values)
    print("Original Q Values:", original_Q)
    print("Original V Values:", original_V)
    print("Original Delta Values:", original_delta)
    print("Original Total Power:", env.total_power)

    print("\nAttacked P Values:", env.current_p_values)
    print("Attacked Q Values:", attacked_Q)
    print("Attacked V Values:", attacked_V)
    print("Attacked Delta Values:", attacked_delta)
    print("Attacked Total Power:", sum(env.current_p_values))

    print("\nP Value Difference:", env.current_p_values - predicted_p_values)
    print("Q Value Difference:", attacked_Q - original_Q)
    print("V Value Difference:", attacked_V - original_V)
    print("Delta Value Difference:", attacked_delta - original_delta)
    print("Total Power Difference:", env.total_power - sum(env.current_p_values))

    print("\nAttack Successful:", env.attack_success_history[-1])

    # Print defender performance
    if env.enable_defender:
        attacks_detected = sum(env.defender_detection_history)
        total_attempts = len(env.defender_detection_history)
        print(f"\nDefender Performance:")
        print(f"Attacks Detected: {attacks_detected}/{total_attempts} ({100*attacks_detected/total_attempts:.2f}%)")
        print(f"Defender Detection Threshold: {env.defender.detection_threshold}")

    print("\nTraining completed!")


def visualize_results_with_defender(rewards, success_rates, env, original_p, original_q, original_v, original_delta,
                                   attacked_p, attacked_q, attacked_v, attacked_delta):
    """Visualize training results and power system parameters including defender performance"""
    plt.figure(figsize=(18, 12))

    # Plot rewards
    plt.subplot(3, 3, 1)
    plt.plot(rewards)
    plt.title('Episode Rewards')
    plt.xlabel('Episode')
    plt.ylabel('Reward')

    # Plot success rates
    plt.subplot(3, 3, 2)
    plt.plot(success_rates)
    plt.title('Attack Success Rate')
    plt.xlabel('Episode')
    plt.ylabel('Success Rate')

    # Plot defender detection rate
    if env.enable_defender and len(env.defender_detection_history) > 0:
        plt.subplot(3, 3, 3)
        # Calculate moving average of detection rate
        window_size = min(20, len(env.defender_detection_history))
        detection_rates = []
        for i in range(len(env.defender_detection_history) - window_size + 1):
            detection_rates.append(sum(env.defender_detection_history[i:i+window_size]) / window_size)

        plt.plot(detection_rates)
        plt.title('Defender Detection Rate (Moving Avg)')
        plt.xlabel('Step')
        plt.ylabel('Detection Rate')

    # Plot P value distribution
    plt.subplot(3, 3, 4)
    x = np.arange(len(original_p))
    width = 0.35

    plt.bar(x - width/2, original_p, width, label='Original P Values')
    plt.bar(x + width/2, attacked_p, width, label='Attacked P Values')
    plt.title('Active Power (P) Values Before and After Attack')
    plt.xlabel('Bus Index')
    plt.ylabel('Active Power (MW)')
    plt.xticks(x, [f'Bus {i}' for i in range(len(original_p))])
    plt.legend()

    # Plot Q value distribution
    plt.subplot(3, 3, 5)
    x = np.arange(len(original_q))
    width = 0.35

    plt.bar(x - width/2, original_q, width, label='Original Q Values')
    plt.bar(x + width/2, attacked_q, width, label='Attacked Q Values')
    plt.title('Reactive Power (Q) Values Before and After Attack')
    plt.xlabel('Bus Index')
    plt.ylabel('Reactive Power (MVAR)')
    plt.xticks(x, [f'Bus {i}' for i in range(len(original_q))])
    plt.legend()

    # Plot V value distribution
    plt.subplot(3, 3, 6)
    x = np.arange(len(original_v))
    width = 0.35

    plt.bar(x - width/2, original_v, width, label='Original V Values')
    plt.bar(x + width/2, attacked_v, width, label='Attacked V Values')
    plt.title('Voltage Magnitude (V) Values Before and After Attack')
    plt.xlabel('Bus Index')
    plt.ylabel('Voltage (p.u.)')
    plt.xticks(x, [f'Bus {i}' for i in range(len(original_v))])
    plt.legend()

    # Plot delta value distribution
    plt.subplot(3, 3, 7)
    x = np.arange(len(original_delta))
    width = 0.35

    plt.bar(x - width/2, original_delta, width, label='Original Delta Values')
    plt.bar(x + width/2, attacked_delta, width, label='Attacked Delta Values')
    plt.title('Voltage Angle (Delta) Values Before and After Attack')
    plt.xlabel('Bus Index')
    plt.ylabel('Voltage Angle (degrees)')
    plt.xticks(x, [f'Bus {i}' for i in range(len(original_delta))])
    plt.legend()

    # Plot defender loss history if enabled
    if env.enable_defender and len(env.defender.loss_history) > 0:
        plt.subplot(3, 3, 8)
        plt.plot(env.defender.loss_history)
        plt.title('Defender Training Loss')
        plt.xlabel('Training Step')
        plt.ylabel('Loss')

        plt.subplot(3, 3, 9)
        # Plot attack success rate vs detection rate
        if len(success_rates) > 0 and len(env.defender_detection_history) > 0:
            # Aggregate data points
            detection_segments = []
            success_segments = []
            segment_size = max(1, len(env.defender_detection_history) // 20)

            for i in range(0, len(env.defender_detection_history), segment_size):
                end_idx = min(i + segment_size, len(env.defender_detection_history))
                if end_idx > i:
                    detection_segments.append(sum(env.defender_detection_history[i:end_idx]) / (end_idx - i))

            for i in range(0, len(success_rates), segment_size):
                end_idx = min(i + segment_size, len(success_rates))
                if end_idx > i:
                    success_segments.append(sum(success_rates[i:end_idx]) / (end_idx - i))

            min_len = min(len(detection_segments), len(success_segments))
            if min_len > 0:
                plt.scatter(detection_segments[:min_len], success_segments[:min_len], alpha=0.7)
                plt.title('Attack Success vs. Defender Detection')
                plt.xlabel('Detection Rate')
                plt.ylabel('Attack Success Rate')

    plt.tight_layout()
    plt.show()


if __name__ == "__main__":
    main_with_defender()