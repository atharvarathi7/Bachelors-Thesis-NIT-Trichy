# -*- coding: utf-8 -*-
"""168_multi_DL_AM_Filters.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xbGUIP0kAKFl5tIypophpdpY-xXpgWMg
"""

import pandas as pd
from math import sqrt
from numpy import concatenate
from pandas import read_csv
import datetime
from pandas import DataFrame
from pandas import concat
from sklearn.preprocessing import LabelEncoder
import numpy as np
import random
from keras import Model
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras import regularizers
from tensorflow.keras.layers import Dense, Flatten, Dropout, LSTM, Conv1D, MaxPooling1D, SimpleRNN, TimeDistributed, Layer, Input, multiply, Concatenate,Activation, dot
from tensorflow.keras.optimizers import SGD, Adam
import tensorflow.keras.backend as K
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error, r2_score, mean_squared_error
import math
from keras.callbacks import EarlyStopping
import seaborn as sns
from sklearn.covariance import EllipticEnvelope
from sklearn.ensemble import IsolationForest
from sklearn.neighbors import LocalOutlierFactor
from sklearn.svm import OneClassSVM

import IPython

# Add attention layer to the deep learning network
class attention(Layer):
    def __init__(self,return_sequences=True):
        self.return_sequences = return_sequences
        super(attention,self).__init__()

    def build(self,input_shape):
        self.W=self.add_weight(name='attention_weight', shape=(input_shape[-1],1),
                               initializer='random_normal', trainable=True)
        self.b=self.add_weight(name='attention_bias', shape=(input_shape[1],1),
                               initializer='zeros', trainable=True)
        super(attention, self).build(input_shape)

    def call(self,x):
        # Alignment scores. Pass them through tanh function
        e = K.tanh(K.dot(x,self.W)+self.b)
        # Remove dimension of size 1
        e = K.squeeze(e, axis=-1)
        # Compute the weights
        alpha = K.softmax(e)
        # Reshape to tensorFlow format
        alpha = K.expand_dims(alpha, axis=-1)
        # Compute the context vector
        context = x * alpha
        context = K.sum(context, axis=1)
        return context

#import datasets
def parser(x):
   return datetime.datetime.strptime(x, '%m/%d/%Y %H:%M')

# With PJM Dataset
dataset_lf = pd.read_excel('inso_ae_ocsvm.xlsx', index_col=0, header=0, date_parser=parser)
dataset_lf = dataset_lf.drop(['SL','Spv','CI'], axis=1)

# With ERCOT Dataset
#dataset_lf = pd.read_excel('LoadTexasERCOT.xlsx', index_col=0, header=0, date_parser=parser)
#dataset_lf = dataset_lf.drop(['COAST','EAST','FAR_WEST','NORTH','NORTH_C','SOUTHERN','SOUTH_C','WEST'], axis=1)

# With CAISO Dataset
#dataset_lf = pd.read_excel('CAISO.xlsx', index_col=0, header=0, date_parser=parser)

print(dataset_lf.tail())

# convert series to supervised learning
def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):
    n_vars = 1 if type(data) is list else data.shape[1]
    df = DataFrame(data)
    cols, names = list(), list()
    # input sequence (t-n, ... t-1)
    for i in range(n_in, 0, -1):
        cols.append(df.shift(i))
        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]
    # forecast sequence (t, t+1, ... t+n)
    for i in range(0, n_out):
        cols.append(df.shift(-i))
        if i == 0:
            names += [('var%d(t)' % (j+1)) for j in range(n_vars)]
        else:
            names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]
    # put it all together
    agg = concat(cols, axis=1)
    agg.columns = names
    # drop rows with NaN values
    if dropnan:
        agg.dropna(inplace=True)
    return agg

dataset_values_lf = dataset_lf.values
objlist = dataset_lf.select_dtypes(include = "object").columns
print (objlist)
##
### Time of the week encoder
encoder = LabelEncoder()
for feat in objlist:
    dataset_lf[feat] = encoder.fit_transform(dataset_lf[feat].astype(str))

dataset_lf.corr()

lf_mean = np.mean(dataset_lf['Load_W'])
lf_std = np.std(dataset_lf['Load_W'])
print('mean of the dataset is', lf_mean)
print('std. deviation is', lf_std)

# STD Method
upper = dataset_lf['Load_W'].mean() + 3*dataset_lf['Load_W'].std()

lower = dataset_lf['Load_W'].mean() -3*dataset_lf['Load_W'].std()

print(upper)

print(lower)
new_dataset_lf= dataset_lf[(dataset_lf['Load_W']<upper) & (dataset_lf['Load_W']>lower)]
#new_dataset_lf.head()
new_dataset_lf.shape

#Zscore Method
zscore = ( dataset_lf['Load_W'] - dataset_lf['Load_W'].mean() ) / dataset_lf['Load_W'].std()

new_dataset_lf = dataset_lf[(zscore>-3) & (zscore<3)]
new_dataset_lf.shape

#Finding the iqr
Q1 = dataset_lf['Load_W'].quantile(0.25)
Q3 = dataset_lf['Load_W'].quantile(0.75)

#Finding upper and lower limit
upper_limit = Q3 + (1.5 * (Q3-Q1))
lower_limit = Q1 - (1.5 * (Q3-Q1))
##Finding Outliers

ub = dataset_lf[dataset_lf['Load_W'] > upper_limit]
lb = dataset_lf[dataset_lf['Load_W'] < lower_limit]

##Trimming

new_dataset_lf = dataset_lf[(dataset_lf['Load_W'] < upper_limit) & (dataset_lf['Load_W']>lower_limit)]

##Compare the plots after trimming

#plt.subplot(2,2,1)
#sns.boxplot(dataset_lf_new['Insolation(W/m2)'], color='red',orient='v')
#plt.subplot(2,2,2)
#sns.boxplot(dataset_lf_new['Temp'],orient='v')
#plt.subplot(2,2,3)
#sns.boxplot(dataset_lf_new['Humidity'],color='green',orient='v')
#plt.subplot(2,2,4)
#sns.boxplot(dataset_lf_new['Wind'],color='orange',orient='v')
#plt.show()


##Capping
print(dataset_lf.shape)
print(new_dataset_lf.shape)
#Compare the plots after capping


#plt.subplot(2,2,1)
#sns.histplot(dataset_lf['Load_W'])
#plt.subplot(2,2,2)
#sns.boxplot(dataset_lf['Load_W'])
#plt.subplot(2,2,3)
#sns.histplot(dataset_lf_new['Load_W'])
#plt.subplot(2,2,4)
#sns.boxplot(dataset_lf_new['Load_W'])
#plt.show()

plt.figure(figsize=(12, 10))

plt.subplot(2, 2, 1)
dataset_lf['Load_W'].hist(bins=50, label='Load Demand', alpha=0.6)
plt.axvline(np.mean(dataset_lf['Load_W']), ls='--', c='r', label="Mean")
plt.axvline(np.median(dataset_lf['Load_W']), ls=':', c='g', label="Median")
plt.ylabel("Counts")
plt.title("Load Demand")
plt.legend()

plt.subplot(2, 2, 2)
plt.scatter(dataset_lf['Load_W'], np.random.normal(7, 0.2, size=dataset_lf.shape[0]), alpha=0.5)
plt.title("Load Demand")

plt.subplot(2, 2, 3)
sns.boxplot(y="Load_W", data=dataset_lf)

plt.subplot(2, 2, 4)
sns.violinplot(y="Load_W", data=dataset_lf, inner="quartile", bw=0.2)

### ensure all data is float
#dataset_values_lf = new_dataset_lf.astype('float32')
dataset_values_lf = dataset_lf.astype('float32')
### normalize features
scaler = MinMaxScaler(feature_range=(0, 1))
dataset_scaled_lf = scaler.fit_transform(dataset_values_lf)
### frame as supervised learning
dataset_reframed_lf = series_to_supervised(dataset_scaled_lf, 168, 1)
### drop columns we don't want to predict
dataset_reframed_lf.drop(dataset_reframed_lf.iloc[:,7:1008], axis=1, inplace=True)

#print(dataset_reframed_lf.head())

dataset_reframed_lf2 = pd.DataFrame(dataset_reframed_lf)

dataset_reframed_lf2.drop(dataset_reframed_lf2.columns[[14,15,16,17,18,19,20,21,22,23,24,25,26,
                                27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,
                                47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,
                                67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,
                                87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,
                                106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,
                                121,122,123,124,125,126,127,128,129,130,131,132,133,
                                134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,
                                150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,
                                                        171,176,177,178,179,180,181]], axis=1, inplace=True)
print(dataset_reframed_lf2.head())
####
### split into train and test sets
dataset_values_lf = dataset_reframed_lf2.values
##random.shuffle(dataset_values_lf)
n_train_hours_lf = int(len(dataset_lf)*0.7)
train_lf = dataset_values_lf[:n_train_hours_lf, :]
test_lf = dataset_values_lf[n_train_hours_lf:, :]
print(train_lf.shape, test_lf.shape)

# split into input and outputs (PJM)
train_X_lf, train_y_lf = train_lf[:, :-1], train_lf[:, -1:]
test_X_lf, test_y_lf = test_lf[:, :-1], test_lf[:, -1:]

# split into input and outputs (CAISO)
#train_X_lf, train_y_lf = train_lf[:, :9], train_lf[:, 9:10]
#test_X_lf, test_y_lf = test_lf[:, :9], test_lf[:, 9:10]

# split into input and outputs (ERCOT)
#train_X_lf, train_y_lf = train_lf[:, :9], train_lf[:, 9:10]
#test_X_lf, test_y_lf = test_lf[:, :9], test_lf[:, 9:10]

#Unsupervised Learning-based Outlier Filters
# MCD
#ee = EllipticEnvelope()
#yhat_lf = ee.fit_predict(train_X_lf)

#IForest
#iso = IsolationForest(contamination=0.1)
#yhat_lf = iso.fit_predict(train_X_lf)

#LOF
#lof = LocalOutlierFactor()
#yhat_lf = lof.fit_predict(train_X_lf)

#One Class SVM
#ee_lf = OneClassSVM(nu=0.01)
#yhat_lf = ee_lf.fit_predict(train_X_lf)

### select all rows that are not outliers
#mask = yhat_lf != -1
#train_X_lf, train_y_lf = train_X_lf[mask, :], train_y_lf[mask]
##### summarize the shape of the updated training dataset
print(train_X_lf.shape, train_y_lf.shape)

### reshape input to be 3D [samples, timesteps, features]
train_X_lf = train_X_lf.reshape((train_X_lf.shape[0], train_X_lf.shape[1], 1))
test_X_lf = test_X_lf.reshape((test_X_lf.shape[0], test_X_lf.shape[1], 1))
print(train_X_lf.shape, train_y_lf.shape, test_X_lf.shape, test_y_lf.shape)

# Create an LSTM network
model_lf = Sequential()
#model_lf.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(train_X_lf.shape[1], train_X_lf.shape[2])))
#model.add(Conv1D(filters=32, kernel_size=5, activation='relu'))
#model_lf.add(MaxPooling1D(pool_size=2))

model_lf.add(LSTM(42, activation='relu', input_shape=(train_X_lf.shape[1], train_X_lf.shape[2]), return_sequences=True))
model_lf.add(attention(return_sequences=False))
model_lf.add(Dense(21, activation='relu'))
model_lf.add(Dense(1))
model_lf.compile(loss='mae', optimizer='adam')

hist_c = model_lf.fit(train_X_lf, train_y_lf, epochs=100, validation_data=(test_X_lf, test_y_lf),
                  verbose=2, callbacks=[EarlyStopping(monitor='val_loss', patience=10, verbose=1)], shuffle=False)

#test_X_lf = test_X_lf.reshape((test_X_lf.shape[0], test_X_lf.shape[1]))
print(test_X_lf.shape)
#print(yhat_lf.shape)
print(test_X_lf[:, 0:1])

yhat_lf = model_lf.predict(test_X_lf)
#yhat_lf = yhat_lf.reshape((yhat_lf.shape[0],yhat_lf.shape[1]))
test_X_lf = test_X_lf.reshape((test_X_lf.shape[0], test_X_lf.shape[1]))
##### invert scaling for forecast
inv_yhat_lf = Concatenate(axis=1)([yhat_lf, test_X_lf[:, -6:]])
inv_yhat_lf_pu = inv_yhat_lf[:,0]

### make a prediction

inv_yhat_lf_pred = scaler.inverse_transform(inv_yhat_lf)
inv_yhat_lf = inv_yhat_lf_pred[:,0]

##### invert scaling for actual
test_y_f = test_y_lf.reshape((len(test_y_lf), 1))
inv_y_lf = Concatenate(axis=1)([test_y_lf, test_X_lf[:, -6:]])
inv_y_lf_pu = inv_y_lf[:,0]
inv_y_lf_act = scaler.inverse_transform(inv_y_lf)
inv_y_lf = inv_y_lf_act[:,0]

inv_y_lf_pu = np.array(inv_y_lf_pu) if hasattr(inv_y_lf_pu, 'numpy') else inv_y_lf_pu
inv_yhat_lf_pu = np.array(inv_yhat_lf_pu) if hasattr(inv_yhat_lf_pu, 'numpy') else inv_yhat_lf_pu
inv_y_lf = np.array(inv_y_lf) if hasattr(inv_y_lf, 'numpy') else inv_y_lf
inv_yhat_lf = np.array(inv_yhat_lf) if hasattr(inv_yhat_lf, 'numpy') else inv_yhat_lf

# calculate RMSE
mae_lf = mean_absolute_error(inv_y_lf_pu, inv_yhat_lf_pu)
print('MAE_lf: %.4f' % mae_lf)
mse_lf = mean_squared_error(inv_y_lf_pu, inv_yhat_lf_pu)
##print('MSE_dataset_lf: %.4f' % mse_dataset_lf)
rmse_lf = math.sqrt(mse_lf)
print('RMSE_lf: %.4f' % rmse_lf)
mbe_lf = np.mean((inv_yhat_lf_pu - inv_y_lf_pu))
print('MBE_lf: %.4f' % mbe_lf)
print('------------------------------------------')

#R-Squared metric
print('R2 Score_lf:', r2_score(inv_y_lf, inv_yhat_lf))
print('------------------------------------------')

fig = plt.figure()
ax = fig.add_axes([0.15, 0.1, 0.8, 0.8])
ax.plot(inv_y_lf[24:48], linewidth = 2.5)
ax.plot(inv_yhat_lf[24:48], linewidth = 2.5)
ax.set_xlabel('Time',fontsize=14, fontweight='bold')
ax.set_ylabel('Load (W)', fontsize=14, fontweight='bold')
ax.set_xticks([0, 4, 8, 12, 16, 20, 23])
ax.grid()
ax.legend(['Actual', 'Predicted'], loc='best')
plt.show()

plt.plot(hist_c.history['loss'], linewidth=2.5, linestyle='solid')
plt.plot(hist_c.history['val_loss'], linewidth=2.5, linestyle='solid')
plt.xlabel('Epochs', fontsize=14, fontweight='bold')
plt.ylabel('Mean Absolute Error', fontsize=14, fontweight='bold')
plt.legend(['Train', 'Val'], loc='best')
plt.grid(True)
plt.show()

pip install xlsxwriter

import xlsxwriter
act = pd.DataFrame(inv_y_lf[:24])
pred = pd.DataFrame(inv_yhat_lf[:24])
train_loss = pd.DataFrame(hist_c.history['loss'])
val_loss = pd.DataFrame(hist_c.history['val_loss'])
writer = pd.ExcelWriter('lf_168_witham.xlsx',
                             engine ='xlsxwriter')

act.to_excel(writer, sheet_name ='Sheet1', startcol = 2)
pred.to_excel(writer, sheet_name ='Sheet1', startcol = 4)
train_loss.to_excel(writer, sheet_name ='Sheet1', startcol = 6)
val_loss.to_excel(writer, sheet_name ='Sheet1', startcol = 8)
writer.save()