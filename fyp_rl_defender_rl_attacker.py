# -*- coding: utf-8 -*-
"""FYP_RL_Defender_RL_Attacker.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vCzV_aTdx9-QmTEvPtlAz_9QqeddRxoK
"""

pip install pandapower

import numpy as np
import pandapower as pp
import gym
from gym import spaces
import torch
import torch.nn as nn
import torch.optim as optim
from torch.distributions import Normal, Categorical
from collections import deque, namedtuple
import random
import matplotlib.pyplot as plt
import warnings
from typing import Tuple, Dict, List, Any, Optional, Union

# Suppress specific warnings that might occur with PyTorch operations
warnings.filterwarnings("ignore", category=UserWarning)

# Define Transition named tuples once
Transition = namedtuple('Transition',
                      ('state', 'action', 'log_prob', 'reward', 'next_state', 'done'))


class BaseNetwork(nn.Module):
    """Base neural network class to reduce code duplication"""

    def __init__(self):
        super(BaseNetwork, self).__init__()

    def save(self, path):
        """Save model parameters"""
        torch.save(self.state_dict(), path)

    def load(self, path):
        """Load model parameters"""
        self.load_state_dict(torch.load(path))


class Actor(BaseNetwork):
    """Improved actor network with better initialization"""

    def __init__(self, state_dim: int, action_dim: int, hidden_dim: int = 128):
        super(Actor, self).__init__()
        # Larger network with better initialization
        self.actor = nn.Sequential(
            nn.Linear(state_dim, hidden_dim),
            nn.Tanh(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.Tanh()
        )

        # Initialize weights using Kaiming initialization
        for layer in self.actor:
            if isinstance(layer, nn.Linear):
                nn.init.kaiming_normal_(layer.weight, nonlinearity='tanh')
                nn.init.constant_(layer.bias, 0)

        # Mean and log_std are separate outputs
        self.mean = nn.Linear(hidden_dim, action_dim)
        nn.init.xavier_uniform_(self.mean.weight)
        nn.init.constant_(self.mean.bias, 0)

        # Initialize log_std with a slightly lower value for more deterministic initial behavior
        self.log_std = nn.Parameter(torch.ones(action_dim) * -1)

    def forward(self, state: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
        x = self.actor(state)
        mean = self.mean(x)

        # Tighter bounds for more precise control
        mean = torch.clamp(mean, -0.5, 0.5)

        # More controlled std
        std = torch.exp(self.log_std.clamp(-20, 2))
        return mean, std


class Critic(BaseNetwork):
    """Improved critic network"""

    def __init__(self, state_dim: int, hidden_dim: int = 128):
        super(Critic, self).__init__()
        self.critic = nn.Sequential(
            nn.Linear(state_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, 1)
        )

        # Initialize weights
        for layer in self.critic:
            if isinstance(layer, nn.Linear):
                nn.init.kaiming_normal_(layer.weight, nonlinearity='relu')
                nn.init.constant_(layer.bias, 0)

    def forward(self, state: torch.Tensor) -> torch.Tensor:
        return self.critic(state)


class DefenderActor(BaseNetwork):
    """Improved defender actor network"""

    def __init__(self, state_dim: int, hidden_dim: int = 128):
        super(DefenderActor, self).__init__()
        self.actor = nn.Sequential(
            nn.Linear(state_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, 2)  # Binary action: (0) No attack, (1) Attack detected
        )

        # Initialize weights
        for layer in self.actor:
            if isinstance(layer, nn.Linear):
                nn.init.kaiming_normal_(layer.weight, nonlinearity='relu')
                nn.init.constant_(layer.bias, 0)

    def forward(self, state: torch.Tensor) -> torch.Tensor:
        x = self.actor(state)
        return torch.softmax(x, dim=-1)


class BasePPO:
    """Base PPO implementation to reduce duplicate code"""

    def __init__(self, gamma: float = 0.99, clip_ratio: float = 0.2,
                 value_coef: float = 0.5, entropy_coef: float = 0.01,
                 max_grad_norm: float = 0.5, batch_size: int = 64,
                 buffer_size: int = 2000):
        self.gamma = gamma
        self.clip_ratio = clip_ratio
        self.value_coef = value_coef
        self.entropy_coef = entropy_coef
        self.max_grad_norm = max_grad_norm
        self.batch_size = batch_size

        # Experience buffer with unified type
        self.buffer = deque(maxlen=buffer_size)

        # Training metrics
        self.actor_losses = []
        self.critic_losses = []
        self.entropy_losses = []

    def compute_returns(self, rewards: List[float], dones: List[bool]) -> torch.Tensor:
        """Compute returns using GAE (Generalized Advantage Estimation)"""
        returns = []
        gae = 0
        next_value = 0  # For terminal states

        # Use reversed indices for more efficient computation
        for i in reversed(range(len(rewards))):
            delta = rewards[i] + self.gamma * next_value * (1 - dones[i]) - next_value
            gae = delta + self.gamma * 0.95 * (1 - dones[i]) * gae  # 0.95 is GAE lambda
            returns.insert(0, gae + next_value)

        returns = torch.FloatTensor(returns)

        # Normalize returns for stable learning
        if len(returns) > 1 and returns.std() > 0:
            returns = (returns - returns.mean()) / (returns.std() + 1e-8)

        return returns

    def store_transition(self, transition: Transition) -> None:
        """Store a transition in the replay buffer"""
        self.buffer.append(transition)

    def save(self, path: str) -> None:
        """Save model parameters"""
        torch.save({
            'actor': self.actor.state_dict(),
            'critic': self.critic.state_dict(),
            'actor_optimizer': self.actor_optimizer.state_dict(),
            'critic_optimizer': self.critic_optimizer.state_dict(),
        }, path)

    def load(self, path: str) -> None:
        """Load model parameters"""
        checkpoint = torch.load(path)
        self.actor.load_state_dict(checkpoint['actor'])
        self.critic.load_state_dict(checkpoint['critic'])
        self.actor_optimizer.load_state_dict(checkpoint['actor_optimizer'])
        self.critic_optimizer.load_state_dict(checkpoint['critic_optimizer'])


class PPO(BasePPO):
    """Optimized PPO implementation for the attacker"""

    def __init__(self, state_dim: int, action_dim: int, lr: float = 3e-4, **kwargs):
        super(PPO, self).__init__(**kwargs)

        # Use device for GPU acceleration if available
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

        # Initialize actor and critic networks
        self.actor = Actor(state_dim, action_dim).to(self.device)
        self.critic = Critic(state_dim).to(self.device)

        # Initialize optimizers with better parameters
        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=lr, eps=1e-5)
        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=lr, eps=1e-5)

    def select_action(self, state: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:
        """Select an action using the current policy"""
        state = torch.FloatTensor(state).to(self.device)
        with torch.no_grad():
            mean, std = self.actor(state)

        # Create a Normal distribution and sample from it
        dist = Normal(mean, std)
        action = dist.sample()

        # Calculate log probability
        log_prob = dist.log_prob(action).sum(dim=-1, keepdim=True)

        # Clip action to be within allowed range
        action = torch.clamp(action, -0.01, 0.01)

        return action.cpu().numpy(), log_prob.cpu().numpy()

    def update(self) -> None:
        """Update the policy and value networks using PPO with optimized batch processing"""
        if len(self.buffer) < self.batch_size:
            return

        # Sample batch
        batch = random.sample(self.buffer, self.batch_size)

        # Extract batch data and convert to tensors directly
        states = torch.FloatTensor(np.array([t.state for t in batch])).to(self.device)
        actions = torch.FloatTensor(np.array([t.action for t in batch])).to(self.device)
        old_log_probs = torch.FloatTensor(np.array([t.log_prob for t in batch])).to(self.device)
        rewards = torch.FloatTensor(np.array([t.reward for t in batch])).to(self.device)
        dones = torch.FloatTensor(np.array([t.done for t in batch])).to(self.device)

        # Compute returns with GAE
        returns = self.compute_returns([t.reward for t in batch], [t.done for t in batch]).to(self.device)

        # Get current value estimates
        values = self.critic(states).squeeze()

        # Compute advantage
        advantages = returns - values.detach()

        # Normalize advantages for stable training
        if advantages.std() > 0:
            advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)

        # Optimize for multiple epochs with mini-batches
        for _ in range(3):  # Reduced from 5 to 3 epochs for efficiency
            # Get current policy distribution
            mean, std = self.actor(states)
            dist = Normal(mean, std)

            # Get log probabilities of actions
            curr_log_probs = dist.log_prob(actions).sum(dim=1)

            # Calculate entropy for exploration
            entropy = dist.entropy().mean()

            # Compute PPO ratio
            ratio = torch.exp(curr_log_probs - old_log_probs.squeeze())

            # Compute PPO losses - clipped surrogate objective
            surr1 = ratio * advantages
            surr2 = torch.clamp(ratio, 1.0 - self.clip_ratio, 1.0 + self.clip_ratio) * advantages

            # Policy loss with advantage normalization
            actor_loss = -torch.min(surr1, surr2).mean()

            # Value loss with clipping for stability
            values = self.critic(states).squeeze()
            value_clipped = values.detach() + torch.clamp(values - values.detach(), -self.clip_ratio, self.clip_ratio)
            value_loss_1 = (values - returns) ** 2
            value_loss_2 = (value_clipped - returns) ** 2
            critic_loss = torch.max(value_loss_1, value_loss_2).mean()

            # Calculate combined loss with adaptive entropy coefficient
            loss = actor_loss + self.value_coef * critic_loss - self.entropy_coef * entropy

            # Update networks with separate optimizers for more stable training
            self.actor_optimizer.zero_grad()
            actor_loss.backward()
            torch.nn.utils.clip_grad_norm_(self.actor.parameters(), self.max_grad_norm)
            self.actor_optimizer.step()

            self.critic_optimizer.zero_grad()
            critic_loss.backward()
            torch.nn.utils.clip_grad_norm_(self.critic.parameters(), self.max_grad_norm)
            self.critic_optimizer.step()

            # Store losses for tracking
            self.actor_losses.append(actor_loss.item())
            self.critic_losses.append(critic_loss.item())
            self.entropy_losses.append(entropy.item())


class DefenderPPO(BasePPO):
    """Optimized PPO implementation for the defender"""

    def __init__(self, state_dim: int, lr: float = 3e-4, **kwargs):
        super(DefenderPPO, self).__init__(**kwargs)

        # Use device for GPU acceleration if available
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

        # Initialize actor and critic networks
        self.actor = DefenderActor(state_dim).to(self.device)
        self.critic = Critic(state_dim).to(self.device)

        # Initialize optimizers
        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=lr, eps=1e-5)
        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=lr, eps=1e-5)

        # Track detection decisions
        self.detection_history = []

    def select_action(self, state: np.ndarray) -> Tuple[int, np.ndarray]:
        """Select an action using the current policy"""
        state = torch.FloatTensor(state).to(self.device)
        with torch.no_grad():
            action_probs = self.actor(state)

        # Create a Categorical distribution and sample from it
        dist = Categorical(action_probs)
        action = dist.sample()

        # Calculate log probability
        log_prob = dist.log_prob(action).unsqueeze(-1)

        return action.item(), log_prob.cpu().numpy()

    def update(self) -> None:
        """Update the policy and value networks using PPO"""
        if len(self.buffer) < self.batch_size:
            return

        # Sample batch
        batch = random.sample(self.buffer, self.batch_size)

        # Extract batch data and convert to tensors directly
        states = torch.FloatTensor(np.array([t.state for t in batch])).to(self.device)
        actions = torch.LongTensor(np.array([t.action for t in batch])).to(self.device)
        old_log_probs = torch.FloatTensor(np.array([t.log_prob for t in batch])).to(self.device)
        rewards = torch.FloatTensor(np.array([t.reward for t in batch])).to(self.device)
        dones = torch.FloatTensor(np.array([t.done for t in batch])).to(self.device)

        # Compute returns with GAE
        returns = self.compute_returns([t.reward for t in batch], [t.done for t in batch]).to(self.device)

        # Get current value estimates
        values = self.critic(states).squeeze()

        # Compute advantage
        advantages = returns - values.detach()

        # Normalize advantages
        if advantages.std() > 0:
            advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)

        # Optimize in mini-batches
        for _ in range(3):  # Reduced from 5 to 3 epochs for efficiency
            # Get current policy distribution
            action_probs = self.actor(states)
            dist = Categorical(action_probs)

            # Get log probabilities of actions
            curr_log_probs = dist.log_prob(actions).unsqueeze(1)

            # Calculate entropy for exploration
            entropy = dist.entropy().mean()

            # Compute PPO ratio
            ratio = torch.exp(curr_log_probs - old_log_probs)

            # Compute PPO losses with clipping
            surr1 = ratio * advantages.unsqueeze(1)
            surr2 = torch.clamp(ratio, 1.0 - self.clip_ratio, 1.0 + self.clip_ratio) * advantages.unsqueeze(1)

            # Policy loss
            actor_loss = -torch.min(surr1, surr2).mean()

            # Value loss with clipping
            values = self.critic(states).squeeze()
            value_clipped = values.detach() + torch.clamp(values - values.detach(), -self.clip_ratio, self.clip_ratio)
            value_loss_1 = (values - returns) ** 2
            value_loss_2 = (value_clipped - returns) ** 2
            critic_loss = torch.max(value_loss_1, value_loss_2).mean()

            # Update networks separately
            self.actor_optimizer.zero_grad()
            (actor_loss - self.entropy_coef * entropy).backward()
            torch.nn.utils.clip_grad_norm_(self.actor.parameters(), self.max_grad_norm)
            self.actor_optimizer.step()

            self.critic_optimizer.zero_grad()
            (self.value_coef * critic_loss).backward()
            torch.nn.utils.clip_grad_norm_(self.critic.parameters(), self.max_grad_norm)
            self.critic_optimizer.step()

            # Store losses for tracking
            self.actor_losses.append(actor_loss.item())
            self.critic_losses.append(critic_loss.item())
            self.entropy_losses.append(entropy.item())


class AdversarialPowerSystemEnv(gym.Env):
    """
    Optimized environment for adversarial interaction between attacker and defender
    in a power system with improved physics modeling and metrics tracking.
    """

    def __init__(self, initial_p_values: np.ndarray, target_indices: Optional[List[int]] = None,
                 episode_length: int = 100):
        super(AdversarialPowerSystemEnv, self).__init__()

        # Store the predicted P values
        self.original_p_values = np.array(initial_p_values, dtype=np.float32)
        self.current_p_values = self.original_p_values.copy()

        # If no target indices provided, target all values
        if target_indices is None:
            self.target_indices = list(range(len(initial_p_values)))
        else:
            self.target_indices = target_indices

        self.num_targets = len(self.target_indices)
        self.episode_length = episode_length
        self.current_step = 0

        # Calculate total power
        self.total_power = np.sum(self.original_p_values)

        # Define action space for the attacker: inject false data to each target (percentage change)
        self.attacker_action_space = spaces.Box(
            low=-0.01,
            high=0.01,
            shape=(self.num_targets,),
            dtype=np.float32
        )

        # Define action space for the defender: binary decision (0: no attack, 1: attack detected)
        self.defender_action_space = spaces.Discrete(2)

        # Define observation space for the attacker
        self.attacker_observation_space = spaces.Box(
            low=0,
            high=np.inf,
            shape=(self.num_targets,),
            dtype=np.float32
        )

        # Calculate initial Q, V, and delta values using the improved model
        self.original_q_values, self.original_v_values, self.original_delta_values = self.calculate_q_v_delta()
        self.current_q_values = self.original_q_values.copy()
        self.current_v_values = self.original_v_values.copy()
        self.current_delta_values = self.original_delta_values.copy()

        # Define a more comprehensive observation space for the defender
        defender_obs_dim = (len(initial_p_values) * 4) + 4  # P, Q, V, delta values + stats
        self.defender_observation_space = spaces.Box(
            low=-np.inf,
            high=np.inf,
            shape=(defender_obs_dim,),
            dtype=np.float32
        )

        # Initialize metric tracking with numpy arrays for efficiency
        self.max_metrics_len = episode_length * 100  # Allow for many episodes
        self.attack_success_history = np.zeros(self.max_metrics_len, dtype=bool)
        self.power_system_success_history = np.zeros(self.max_metrics_len, dtype=bool)
        self.true_attack_history = np.zeros(self.max_metrics_len, dtype=bool)
        self.detection_history = np.zeros(self.max_metrics_len, dtype=bool)
        self.attacker_reward_history = np.zeros(self.max_metrics_len)
        self.defender_reward_history = np.zeros(self.max_metrics_len)
        self.attack_magnitude_history = np.zeros(self.max_metrics_len)

        # Current index for metrics
        self.metrics_idx = 0

        # Performance metrics
        self.true_positives = 0
        self.false_positives = 0
        self.true_negatives = 0
        self.false_negatives = 0

    def reset(self) -> Tuple[np.ndarray, np.ndarray]:
        """Reset the environment to initial state"""
        self.current_step = 0
        self.current_p_values = self.original_p_values.copy()
        self.current_q_values = self.original_q_values.copy()
        self.current_v_values = self.original_v_values.copy()
        self.current_delta_values = self.original_delta_values.copy()

        # Reset the last defender action (no attack detected by default)
        self.last_defender_action = 0

        # Return initial observations for both agents
        return self._get_attacker_observation(), self._get_defender_observation()

    def _get_attacker_observation(self) -> np.ndarray:
        """Get current P values for targets as attacker observation"""
        return self.current_p_values[self.target_indices].copy()

    def _get_defender_observation(self) -> np.ndarray:
        """Get comprehensive system state as defender observation"""
        # Vectorized operation for better performance
        features = np.concatenate([
            self.current_p_values,
            self.current_q_values,
            self.current_v_values,
            self.current_delta_values,
            [np.sum(self.current_p_values)],
            [np.sum(self.current_q_values)],
            [np.mean(self.current_v_values)],
            [np.std(self.current_delta_values)]
        ])
        return features

    def calculate_q_v_delta(self) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:
        """Calculate Q, V, and delta values based on current P values with improved model"""
        return calculate_q_v_delta_enhanced(self.current_p_values)

    def _is_real_attack(self, detection_closeness: np.ndarray) -> bool:
        """Determine if a real attack is occurring based on value deviation"""
        return np.any(detection_closeness > 1.0)

    def _calculate_attacker_reward(self, defender_action: int) -> float:
        """Calculate reward for the attacker with improved incentives"""
        # Extract current values at target indices
        current_targets = self.current_p_values[self.target_indices]
        original_targets = self.original_p_values[self.target_indices]

        # Determine maximum allowable change without obvious detection (now 25% for more flexibility)
        max_allowed_deviation = 0.15 * original_targets

        # Calculate how close we are to the detection threshold
        detection_closeness = np.abs(current_targets - original_targets) / max_allowed_deviation

        # Store attack magnitude for metrics
        attack_magnitude = np.mean(detection_closeness)
        if self.metrics_idx < self.max_metrics_len:
            self.attack_magnitude_history[self.metrics_idx] = attack_magnitude

        # Is this actually an attack based on our thresholds? (ground truth)
        is_real_attack = self._is_real_attack(detection_closeness)
        if self.metrics_idx < self.max_metrics_len:
            self.true_attack_history[self.metrics_idx] = is_real_attack

        # Basic reward components
        # 1. Reward for successful manipulation (how much we've changed values)
        manipulation_reward = 20 * np.sum(np.abs(current_targets - original_targets)) / np.sum(original_targets)

        # 2. Improved stealth rewards/penalties with smoother scaling
        stealth_reward = 8 * np.sum(np.clip(1.0 - detection_closeness, 0, 1))
        detection_penalty = -20 * np.sum(np.maximum(detection_closeness - 1.0, 0)**2)  # Quadratic penalty

        # 3. Power conservation penalty with smoother scaling
        total_current_power = np.sum(self.current_p_values)
        power_deviation = abs(total_current_power - self.total_power) / self.total_power
        power_conservation_penalty = -60 * power_deviation**2  # Quadratic penalty

        # Success metrics
        power_system_success = (np.all(detection_closeness <= 1.0) and
                         power_deviation < 0.01)
        self.power_system_success_history[self.metrics_idx] = power_system_success

        # 4. Large penalty if defender detects the attack
        defender_detection_penalty = -120 if defender_action == 1 else 0

        # 5. Reward for evading detection during actual attack (increased)
        evasion_reward = 60 if (power_system_success and defender_action == 0) else 0

        # Combine rewards with better balancing
        reward = (manipulation_reward +
                 stealth_reward +
                 detection_penalty +
                 power_conservation_penalty +
                 defender_detection_penalty +
                 evasion_reward)

        # Track attack success: attack succeeded and wasn't detected
        attack_success = power_system_success and defender_action == 0
        if self.metrics_idx < self.max_metrics_len:
            self.attack_success_history[self.metrics_idx] = attack_success

        # For metric tracking - update confusion matrix
        if is_real_attack:
            if defender_action == 1:
                self.true_positives += 1
            else:
                self.false_negatives += 1
        else:
            if defender_action == 1:
                self.false_positives += 1
            else:
                self.true_negatives += 1

        # Record rewards
        if self.metrics_idx < self.max_metrics_len:
            self.attacker_reward_history[self.metrics_idx] = reward

        return reward

    def _calculate_defender_reward(self, defender_action: int) -> float:
        """Calculate reward for the defender with better scoring system"""
        # Get the ground truth about whether this is an attack
        is_real_attack = self.true_attack_history[self.metrics_idx] if self.metrics_idx > 0 else False

        reward = 0

        # True positive: correctly identified attack
        if is_real_attack and defender_action == 1:
            # Reward scales with attack magnitude
            attack_magnitude = self.attack_magnitude_history[self.metrics_idx]
            reward += 100 + 50 * attack_magnitude  # Higher reward for catching bigger attacks

        # False positive: flagged normal operation as attack
        elif not is_real_attack and defender_action == 1:
            reward -= 60  # Increased penalty

        # False negative: missed an attack
        elif is_real_attack and defender_action == 0:
            # Higher penalty for missing more severe attacks
            attack_magnitude = self.attack_magnitude_history[self.metrics_idx]
            reward -= 120 * attack_magnitude

        # True negative: correctly identified normal operation
        else:  # not is_real_attack and defender_action == 0:
            reward += 15  # Slightly increased baseline reward

        # Record defender reward
        if self.metrics_idx < self.max_metrics_len:
            self.defender_reward_history[self.metrics_idx] = reward

        # Also record defender's decision
        if self.metrics_idx < self.max_metrics_len:
            self.detection_history[self.metrics_idx] = (defender_action == 1)

        return reward

    def step(self, attacker_action: np.ndarray, defender_action: int) -> Tuple[np.ndarray, np.ndarray, float, float, bool, Dict[str, Any]]:
        """Execute actions from both agents"""
        self.current_step += 1
        self.metrics_idx += 1  # Increment metrics index

        # Apply attacker's action to modify P values with vectorized operations
        changes = self.current_p_values[self.target_indices] * attacker_action
        self.current_p_values[self.target_indices] += changes

        # Ensure all power values are positive with a minimum floor
        self.current_p_values = np.maximum(self.current_p_values, 0.01)

        # Adjust power values proportionally to maintain total power
        total_current_power = np.sum(self.current_p_values)
        power_difference = self.total_power - total_current_power
        if total_current_power > 0:  # Avoid division by zero
            for i, idx in enumerate(self.target_indices):
             self.current_p_values[idx] += power_difference * (self.current_p_values[idx] / total_current_power)

        # Calculate new Q, V, and delta values based on modified P values
        self.current_q_values, self.current_v_values, self.current_delta_values = self.calculate_q_v_delta()

        # Calculate rewards for both agents
        attacker_reward = self._calculate_attacker_reward(defender_action)
        defender_reward = self._calculate_defender_reward(defender_action)

        # Get new observations
        attacker_obs = self._get_attacker_observation()
        defender_obs = self._get_defender_observation()

        # Check if episode is done
        done = self.current_step >= self.episode_length

# Info dictionary with additional data
        info = {
            "current_p_values": self.current_p_values.copy(),
            "current_q_values": self.current_q_values.copy(),
            "current_v_values": self.current_v_values.copy(),
            "current_delta_values": self.current_delta_values.copy(),
            "is_real_attack": self.true_attack_history[self.metrics_idx] if self.metrics_idx > 0 else False,
            "defender_detected": defender_action == 1,
            "attack_succeeded": self.attack_success_history[self.metrics_idx] if self.metrics_idx > 0 else False,
            "attack_magnitude": self.attack_magnitude_history[self.metrics_idx] if self.metrics_idx > 0 else 0.0,
            "confusion_matrix": {
                "true_positives": self.true_positives,
                "false_positives": self.false_positives,
                "true_negatives": self.true_negatives,
                "false_negatives": self.false_negatives
            }
        }

        return attacker_obs, defender_obs, attacker_reward, defender_reward, done, info

    def get_metrics(self) -> Dict[str, Any]:
        """Get performance metrics for the episode"""
        if self.metrics_idx == 0:
            return {"no_data": True}

        # Calculate metrics only on valid data points
        valid_idx = min(self.metrics_idx, self.max_metrics_len)

        # Detection accuracy metrics
        true_attacks = np.sum(self.true_attack_history[:valid_idx])
        true_detections = np.sum(self.detection_history[:valid_idx] & self.true_attack_history[:valid_idx])
        false_detections = np.sum(self.detection_history[:valid_idx] & ~self.true_attack_history[:valid_idx])

        # Attack success metrics
        attack_success_rate = np.mean(self.attack_success_history[:valid_idx]) if true_attacks > 0 else 0

        # Calculate precision, recall, F1 if there are actual attacks
        if true_attacks > 0 and np.sum(self.detection_history[:valid_idx]) > 0:
            precision = true_detections / np.sum(self.detection_history[:valid_idx])
            recall = true_detections / true_attacks
            f1_score = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0
        else:
            precision = recall = f1_score = 0

        # Average rewards
        avg_attacker_reward = np.mean(self.attacker_reward_history[:valid_idx])
        avg_defender_reward = np.mean(self.defender_reward_history[:valid_idx])

        # Confusion matrix
        if self.true_positives + self.false_positives > 0:
            precision_cm = self.true_positives / (self.true_positives + self.false_positives)
        else:
            precision_cm = 0

        if self.true_positives + self.false_negatives > 0:
            recall_cm = self.true_positives / (self.true_positives + self.false_negatives)
        else:
            recall_cm = 0

        # Calculate F1 from precision and recall
        if precision_cm + recall_cm > 0:
            f1_cm = 2 * precision_cm * recall_cm / (precision_cm + recall_cm)
        else:
            f1_cm = 0

        # Accuracy
        total_samples = self.true_positives + self.true_negatives + self.false_positives + self.false_negatives
        accuracy = (self.true_positives + self.true_negatives) / total_samples if total_samples > 0 else 0

        return {
            "attack_success_rate": float(attack_success_rate),
            "detection_rate": float(recall),
            "false_positive_rate": float(false_detections / valid_idx),
            "precision": float(precision),
            "recall": float(recall),
            "f1_score": float(f1_score),
            "avg_attacker_reward": float(avg_attacker_reward),
            "avg_defender_reward": float(avg_defender_reward),
            "confusion_matrix": {
                "true_positives": int(self.true_positives),
                "false_positives": int(self.false_positives),
                "true_negatives": int(self.true_negatives),
                "false_negatives": int(self.false_negatives),
                "precision": float(precision_cm),
                "recall": float(recall_cm),
                "f1_score": float(f1_cm),
                "accuracy": float(accuracy)
            }
        }

    def render(self, mode: str = 'human') -> None:
        """Render the environment metrics"""
        if mode != 'human':
            return

        metrics = self.get_metrics()
        if "no_data" in metrics:
            print("No metrics data available yet.")
            return

        print(f"Attack Success Rate: {metrics['attack_success_rate']:.4f}")
        print(f"Detection Rate: {metrics['detection_rate']:.4f}")
        print(f"False Positive Rate: {metrics['false_positive_rate']:.4f}")
        print(f"F1 Score: {metrics['f1_score']:.4f}")

        # Print confusion matrix stats
        cm = metrics["confusion_matrix"]
        print("\nConfusion Matrix:")
        print(f"True Positives: {cm['true_positives']}, False Positives: {cm['false_positives']}")
        print(f"True Negatives: {cm['true_negatives']}, False Negatives: {cm['false_negatives']}")
        print(f"Precision: {cm['precision']:.4f}, Recall: {cm['recall']:.4f}")
        print(f"F1 Score: {cm['f1_score']:.4f}, Accuracy: {cm['accuracy']:.4f}")

def calculate_q_v_delta_enhanced(p_values: np.ndarray) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:
    """
    Calculate reactive power (Q), voltage magnitude (V), and voltage angle (delta)
    for a simplified network where all load buses are directly connected to the slack bus.

    Parameters:
    p_values (numpy.ndarray): Active power values for all load buses

    Returns:
    tuple: (Q_values, V_values, delta_values)
    """
    if not isinstance(p_values, np.ndarray):
        raise ValueError("p_values must be a numpy array")

    # Create an empty network
    net = pp.create_empty_network()

    # Add a slack bus (within the DSO)
    slack_bus = pp.create_bus(net, vn_kv=230, name="Slack Bus")
    pp.create_ext_grid(net, bus=slack_bus, vm_pu=1.0, name="Grid Connection")

    # Create load buses
    buses = [pp.create_bus(net, vn_kv=230, name=f"Bus {i+1}") for i in range(len(p_values))]

    # Connect each load bus directly to the slack bus
    for i, bus in enumerate(buses):
        pp.create_line_from_parameters(
            net, from_bus=slack_bus, to_bus=bus, length_km=1,
            r_ohm_per_km=0.02, x_ohm_per_km=0.06, c_nf_per_km=0, max_i_ka=0.5,
            name=f"Slack to Bus {i+1}"
        )

    # Calculate total active power demand
    total_p = np.sum(p_values[p_values > 0])

    # Assume constant power factor of 0.5
    power_factor = 0.5
    q_factor = np.tan(np.arccos(power_factor))

    # Add loads to the buses
    for i, p_mw in enumerate(p_values):
        if p_mw <= 0:
            continue  # Skip buses with non-positive power

        q_mvar = p_mw * q_factor

        pp.create_load(net, bus=buses[i], p_mw=p_mw, q_mvar=q_mvar, name=f"Load at Bus {i+1}")

    # Set slack bus power injection
    pp.create_gen(net, bus=slack_bus, p_mw=total_p, vm_pu=1.0, name="Slack Generator")

    # Run power flow
    try:
        pp.runpp(net)
    except Exception as e:
        print(f"Power flow calculation failed: {e}")
        return (
            np.full_like(p_values, np.nan),
            np.full_like(p_values, np.nan),
            np.full_like(p_values, np.nan),
        )

     # Extract results for load buses only
    load_bus_indices = [bus for bus in buses]
    Q_values = net.res_load.q_mvar.values if "res_load" in net else np.zeros_like(p_values)
    V_values = net.res_bus.vm_pu.loc[load_bus_indices].values
    delta_values = net.res_bus.va_degree.loc[load_bus_indices].values

    return Q_values, V_values, delta_values

class AdversarialTraining:
    """
    Class to manage the adversarial training of attackers and defenders
    in the power system environment.
    """

    def __init__(self, env: AdversarialPowerSystemEnv,
                 attacker: PPO, defender: DefenderPPO,
                 n_episodes: int = 1000, eval_frequency: int = 50):
        self.env = env
        self.attacker = attacker
        self.defender = defender
        self.n_episodes = n_episodes
        self.eval_frequency = eval_frequency

        # Training metrics
        self.attacker_rewards = []
        self.defender_rewards = []
        self.attack_success_rates = []
        self.detection_rates = []
        self.f1_scores = []

    def train(self, verbose: bool = True, save_path: Optional[str] = None) -> Dict[str, List[float]]:
        """Train both agents in the adversarial environment"""
        print("Starting adversarial training...")

        for episode in range(1, self.n_episodes + 1):
            # Reset environment and get initial observations
            attacker_obs, defender_obs = self.env.reset()

            episode_attacker_rewards = []
            episode_defender_rewards = []

            # Episode loop
            done = False
            while not done:
                # Attacker selects action
                attacker_action, attacker_log_prob = self.attacker.select_action(attacker_obs)

                # Defender selects action
                defender_action, defender_log_prob = self.defender.select_action(defender_obs)

                # Take step in environment
                next_attacker_obs, next_defender_obs, attacker_reward, defender_reward, done, info = self.env.step(attacker_action, defender_action)

                # Store transitions
                self.attacker.store_transition(Transition(
                    attacker_obs, attacker_action, attacker_log_prob, attacker_reward, next_attacker_obs, done
                ))

                self.defender.store_transition(Transition(
                    defender_obs, defender_action, defender_log_prob, defender_reward, next_defender_obs, done
                ))

                # Update observations
                attacker_obs = next_attacker_obs
                defender_obs = next_defender_obs

                # Store rewards
                episode_attacker_rewards.append(attacker_reward)
                episode_defender_rewards.append(defender_reward)

            # Update networks after each episode
            self.attacker.update()
            self.defender.update()

            # Track metrics
            mean_attacker_reward = np.mean(episode_attacker_rewards)
            mean_defender_reward = np.mean(episode_defender_rewards)
            self.attacker_rewards.append(mean_attacker_reward)
            self.defender_rewards.append(mean_defender_reward)

            # Get and store evaluation metrics
            metrics = self.env.get_metrics()
            self.attack_success_rates.append(metrics["attack_success_rate"])
            self.detection_rates.append(metrics["detection_rate"])
            self.f1_scores.append(metrics["f1_score"])

            # Print progress
            if verbose and episode % 10 == 0:
                print(f"Episode {episode}/{self.n_episodes}")
                print(f"Attacker Reward: {mean_attacker_reward:.2f}, Defender Reward: {mean_defender_reward:.2f}")
                print(f"Attack Success Rate: {metrics['attack_success_rate']:.4f}, Detection Rate: {metrics['detection_rate']:.4f}")
                print(f"F1 Score: {metrics['f1_score']:.4f}")
                print("---")

            # Save models periodically
            if save_path and episode % self.eval_frequency == 0:
                self.attacker.save(f"{save_path}/attacker_episode_{episode}.pth")
                self.defender.save(f"{save_path}/defender_episode_{episode}.pth")

        print("Training complete!")

        # Return training history
        return {
            "attacker_rewards": self.attacker_rewards,
            "defender_rewards": self.defender_rewards,
            "attack_success_rates": self.attack_success_rates,
            "detection_rates": self.detection_rates,
            "f1_scores": self.f1_scores
        }

    def evaluate(self, n_episodes: int = 10, render: bool = True) -> Dict[str, float]:
        """Evaluate trained agents without updating policies"""
        print("Starting evaluation...")

        eval_attack_success_rates = []
        eval_detection_rates = []
        eval_f1_scores = []
        eval_attacker_rewards = []
        eval_defender_rewards = []

        for episode in range(1, n_episodes + 1):
            # Reset environment
            attacker_obs, defender_obs = self.env.reset()

            episode_attacker_rewards = []
            episode_defender_rewards = []

            # Episode loop
            done = False
            while not done:
                # Attacker selects action
                attacker_action, _ = self.attacker.select_action(attacker_obs)

                # Defender selects action
                defender_action, _ = self.defender.select_action(defender_obs)

                # Take step in environment
                next_attacker_obs, next_defender_obs, attacker_reward, defender_reward, done, _ = \
                    self.env.step(attacker_action, defender_action)

                # Update observations
                attacker_obs = next_attacker_obs
                defender_obs = next_defender_obs

                # Store rewards
                episode_attacker_rewards.append(attacker_reward)
                episode_defender_rewards.append(defender_reward)

            # Render metrics if requested
            if render:
                print(f"\nEvaluation Episode {episode}/{n_episodes}")
                self.env.render()

            # Get metrics for this episode
            metrics = self.env.get_metrics()
            eval_attack_success_rates.append(metrics["attack_success_rate"])
            eval_detection_rates.append(metrics["detection_rate"])
            eval_f1_scores.append(metrics["f1_score"])
            eval_attacker_rewards.append(np.mean(episode_attacker_rewards))
            eval_defender_rewards.append(np.mean(episode_defender_rewards))

        # Calculate average metrics
        avg_metrics = {
            "attack_success_rate": np.mean(eval_attack_success_rates),
            "detection_rate": np.mean(eval_detection_rates),
            "f1_score": np.mean(eval_f1_scores),
            "attacker_reward": np.mean(eval_attacker_rewards),
            "defender_reward": np.mean(eval_defender_rewards)
        }

        print("\nEvaluation Results:")
        print(f"Average Attack Success Rate: {avg_metrics['attack_success_rate']:.4f}")
        print(f"Average Detection Rate: {avg_metrics['detection_rate']:.4f}")
        print(f"Average F1 Score: {avg_metrics['f1_score']:.4f}")
        print(f"Average Attacker Reward: {avg_metrics['attacker_reward']:.2f}")
        print(f"Average Defender Reward: {avg_metrics['defender_reward']:.2f}")

        return avg_metrics

    def plot_training_history(self) -> None:
        """Plot training metrics history"""
        plt.figure(figsize=(18, 10))

        # Plot rewards
        plt.subplot(2, 2, 1)
        plt.plot(self.attacker_rewards, label='Attacker')
        plt.plot(self.defender_rewards, label='Defender')
        plt.xlabel('Episode')
        plt.ylabel('Average Reward')
        plt.title('Agent Rewards During Training')
        plt.legend()

        # Plot attack success and detection rates
        plt.subplot(2, 2, 2)
        plt.plot(self.attack_success_rates, label='Attack Success Rate')
        plt.plot(self.detection_rates, label='Detection Rate')
        plt.xlabel('Episode')
        plt.ylabel('Rate')
        plt.title('Attack Success vs Detection')
        plt.legend()

        # Plot F1 scores
        plt.subplot(2, 2, 3)
        plt.plot(self.f1_scores)
        plt.xlabel('Episode')
        plt.ylabel('F1 Score')
        plt.title('Defender F1 Score During Training')

        # Plot moving averages for trend visualization
        window = min(50, len(self.attacker_rewards) // 10)
        if window > 0:
            plt.subplot(2, 2, 4)
            attacker_ma = np.convolve(self.attacker_rewards, np.ones(window)/window, mode='valid')
            defender_ma = np.convolve(self.defender_rewards, np.ones(window)/window, mode='valid')
            plt.plot(attacker_ma, label='Attacker (Moving Avg)')
            plt.plot(defender_ma, label='Defender (Moving Avg)')
            plt.xlabel('Episode')
            plt.ylabel('Average Reward (Moving Avg)')
            plt.title(f'Reward Moving Average (Window={window})')
            plt.legend()

        plt.tight_layout()
        plt.show()


def run_example(verbose: bool = True):
    """Run a simple example of the adversarial power system simulation"""
    # Define initial P values (power injections at each bus)
    p_values = predicted_p_values = np.array([10.0, 12.5, 8.0, 15.2, 9.8])

    # Create environment
    env = AdversarialPowerSystemEnv(initial_p_values=p_values, episode_length=50)

    # Create attacker and defender agents
    attacker = PPO(
        state_dim=env.attacker_observation_space.shape[0],
        action_dim=env.attacker_action_space.shape[0],
        lr=3e-4,
        gamma=0.99,
        clip_ratio=0.2,
        batch_size=64
    )

    defender = DefenderPPO(
        state_dim=env.defender_observation_space.shape[0],
        lr=3e-4,
        gamma=0.99,
        clip_ratio=0.2,
        batch_size=64,
        entropy_coef=0.02  # Slightly higher exploration for defender
    )

    # Create trainer
    trainer = AdversarialTraining(
        env=env,
        attacker=attacker,
        defender=defender,
        n_episodes=40,
        eval_frequency=20
    )

    # Train agents
    training_history = trainer.train(verbose=verbose)

    # Evaluate agents
    trainer.evaluate(n_episodes=5, render=True)

    # Plot training history
    trainer.plot_training_history()

    return env, attacker, defender, trainer


if __name__ == "__main__":
    # Set random seeds for reproducibility
    torch.manual_seed(42)
    np.random.seed(42)
    random.seed(42)

    # Run example
    env, attacker, defender, trainer = run_example(verbose=True)